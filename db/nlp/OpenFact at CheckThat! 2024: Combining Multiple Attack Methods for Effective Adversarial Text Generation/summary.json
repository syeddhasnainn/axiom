{"abstract": "", "introduction": "Robustness of credibility assessment is an interesting area of research that helps to understand the limitations of automatic classification methods used in variety of applications. The primary objective of this task was to generate adversarial examples in five problem domains in order to evaluate the robustness of widely used text classification methods (fine-tuned BERT, BiLSTM, and RoBERTa) when applied to credibility assessment issues. This study explores the application of ensemble learning to enhance adversarial attacks on natural language processing (NLP) models. We systematically tested and refined several adversarial attack methods, including BERT-Attack, Genetic algorithms, TextFooler, and CLARE, on five datasets across various misinformation tasks. By developing modified versions of BERT-Attack and hybrid methods, we achieved significant improvements in attack effectiveness. Our results demonstrate the potential of modification and combining multiple methods to create more sophisticated and effective adversarial attack strategies, contributing to the development of more robust and secure systems.", "future_direction": "In future works we plan to extend the methods by using various data sources and large language models (LLMs). For example, as a comprehensive text corpus, Wikipedia can be used to train or fine-tune language models for better contextual understanding and generation. Using Wikipedia articles, the quality of adversarial examples can be further enhanced by providing more natural and contextually appropriate substitutions, particularly when we can consider quality differences between Wikipedia articles in various language versions [22]. Especially, we plan to use publicly available services with measures related to information quality and reliability of sources, such as BestRef [23], WikiRank [24] and various revscoring models [25]. Future research can leverage the analysis of Wikipedia references to enhance adversarial attacks by incorporating high-quality scientific sources into training datasets, ensuring more credible and contextually relevant adversarial examples [26].Such structured knowledge bases as Wikidata and DBpedia can provide rich semantic context and relationships between entities. Integrating those and other data sources [ 27] could help in refine word importance ranking and improve the selection of substitutes by ensuring semantic consistency and relevance. For example, entity linking and relationship extraction from these knowledge bases can ensure that replacements preserve factual accuracy and context. The advancements in adversarial attack strategies can be further developed by integrating insights from the analysis of large language models (LLMs) like ChatGPT on the fake news phenomenon [28]. Utilizing state-of-the-art and novel LLMs, such as Claude, Gemini, GPT-4, LLama, Mistral, can improve the quality of generated adversarial examples. Fine-tuning these models on specific datasets or incorporating them into the ensemble can lead to more sophisticated and effective attacks. LLMs can also be used to simulate human-like understanding and generation of text, making adversarial examples more natural and harder to detect. Moreover, combining traditional adversarial attack methods with the capabilities of LLMs can create hybrid approaches that leverage the strengths of both. For example, initial perturbations can be generated using classical methods, followed by refinement and enhancement using LLMs.", "research_gap": "There are several research gaps that still need to be addressed in the field of adversarial attacks on text classification. Firstly, most existing methods rely on the availability of large amounts of labeled data, which can be a major limitation in real-world scenarios. Secondly, the generated adversarial examples may not always preserve the semantic meaning of the original text, which can lead to inconsistencies in the results. Finally, the existing methods may not be able to handle out-of-distribution inputs, which can be a major limitation in real-world scenarios. To address these research gaps, future work could focus on developing methods that can learn from small amounts of labeled data, preserving the semantic meaning of the original text, and handling out-of-distribution inputs.", "conclusion": "In this study, we explored the application of ensemble learning to the domain of adversarial attacks on text classifiers. Our approach was motivated by the principle that leveraging multiple adversarial attack methods can yield better performance compared to any single method. Through systematic experimentation and modification of existing attack algorithms, we aimed to improve the effectiveness and efficiency of adversarial attacks against various victim models. We began with a literature review and replicated some of the prominent adversarial attack methods, using implementations from the OpenAttack library. Initial experiments indicated that BERT-Attack and Genetic Algorithm were generally effective, though other methods like TextFooler, DeepWordBug, and SCPN also showed promise in specific scenarios. This led us to test these methods on our five datasets: C19, FC, HN, PR2, and RD, initially targeting two victim models: BERT and BiLSTM. Our first set of modifications to BERT-Attack (resulting in the BAm variant) involved adjusting parameters to enhance the balance between semantic preservation and attack success rate. These adjustments improved performance across several metrics, as shown in our comparative analysis. Further, we introduced a novel method for ranking word importance, which led to the BAm2 variant. This method evaluates the impact of potential word substitutions on model predictions, thereby allowing more informed and effective attacks. Additionally, for cases where BAm2 failed, we integrated the Genetic Algorithm to bolster the attack success rate. This combination, BAm2&Genetic, yielded significant improvements, particularly in the BODEGA and success scores. We then explored an alternative approach, GSWSE, which employs word embeddings for synonym replacement. This method, coupled with TextFooler for the challenging cases, offered further enhancements in BODEGA score, albeit with a slight reduction in the success rate. In the final stage of our experiments, we used the CLARE method. CLAREâ€™s innovative mask-then-infill procedure, leveraging a pre-trained masked language model, demonstrated superior performance across all metrics. It provided a high attack success rate while maintaining textual similarity and fluency, making it the most effective method in our ensemble. Finally, the developed solutions were tested on the third victim model: RoBERTa. The evaluation confirmed the effectiveness of our approach in generating adversarial text in five problem domains. Our findings underscore the potential of ensemble learning in the adversarial attack domain. By combining multiple attack strategies and continuously refining our methods, we achieved notable improvements in attack effectiveness. Future work could explore additional ensemble configurations and further optimizations, aiming to develop even more robust adversarial attack frameworks."}