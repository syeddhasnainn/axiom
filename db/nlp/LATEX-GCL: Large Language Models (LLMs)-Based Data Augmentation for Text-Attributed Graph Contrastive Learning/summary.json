{"abstract": "Graph Contrastive Learning (GCL) is a potent paradigm for self-supervised graph learning that has attracted attention across various application scenarios. However, GCL for learning on Text-Attributed Graphs (TAGs) has yet to be explored. In this paper, we propose a novel GCL framework named LATEX-GCL to utilize Large Language Models (LLMs) to produce textual augmentations and LLMsâ€™ powerful natural language processing (NLP) abilities to address the three limitations of conventional GCL methods for TAGs.", "introduction": "In numerous real-world scenarios, graph data is often enriched with textual attributes, for instance, user-item interaction graphs in recommendation systems that include textual user profiles and product descriptions [ 9,16]. This type of graph data is referred to as TAGs [ 3]. More than recommendation systems, the application scenarios of TAGs also include bioinformatics [ 2], computer vision [ 20], and quantum computing [ 11]. The development of effective methodologies for processing and analyzing TAGs is crucial for advancing applications that rely on such data.", "future_direction": "This research is expected to be a pioneering work that encourages the exploration of LLMs for GCL. The future directions are two-fold, including investigating more comprehensive augmentation prompting strategies for different scenarios and how to improve the computation efficiency of employing LLMs in real-world applications.", "research_gap": "Despite the advancements in GCL, the literature reveals a gap in the development of GCL methodologies specifically tailored for TAG settings [ 3,12,24].", "conclusion": "This paper proposes a novel GCL framework, namely LATEX-GCL, which successfully incorporates LLMs to conduct augmentations to construct contrasting samples. The purpose of the proposed augmentation strategy is to leverage the advantages of LLMs to tackle the limitations of information loss, incapable language models, and implicit constraints of current GCL methods for TAGs."}